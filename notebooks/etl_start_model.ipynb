{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month, monotonically_increasing_id\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL to Star Schema\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/work/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pg_url = \"jdbc:postgresql://bigdata_postgres_db:5432/lab2\"\n",
    "pg_props = {\n",
    "    \"user\": \"debug\",\n",
    "    \"password\": \"pswd\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f965d639-3c81-4264-b06f-21e09a3f0ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- customer_first_name: string (nullable = true)\n",
      " |-- customer_last_name: string (nullable = true)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_country: string (nullable = true)\n",
      " |-- customer_postal_code: string (nullable = true)\n",
      " |-- customer_pet_type: string (nullable = true)\n",
      " |-- customer_pet_name: string (nullable = true)\n",
      " |-- customer_pet_breed: string (nullable = true)\n",
      " |-- seller_first_name: string (nullable = true)\n",
      " |-- seller_last_name: string (nullable = true)\n",
      " |-- seller_email: string (nullable = true)\n",
      " |-- seller_country: string (nullable = true)\n",
      " |-- seller_postal_code: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_price: decimal(10,2) (nullable = true)\n",
      " |-- product_quantity: integer (nullable = true)\n",
      " |-- sale_date: timestamp (nullable = true)\n",
      " |-- sale_customer_id: integer (nullable = true)\n",
      " |-- sale_seller_id: integer (nullable = true)\n",
      " |-- sale_product_id: integer (nullable = true)\n",
      " |-- sale_quantity: integer (nullable = true)\n",
      " |-- sale_total_price: decimal(10,2) (nullable = true)\n",
      " |-- store_name: string (nullable = true)\n",
      " |-- store_location: string (nullable = true)\n",
      " |-- store_city: string (nullable = true)\n",
      " |-- store_state: string (nullable = true)\n",
      " |-- store_country: string (nullable = true)\n",
      " |-- store_phone: string (nullable = true)\n",
      " |-- store_email: string (nullable = true)\n",
      " |-- pet_category: string (nullable = true)\n",
      " |-- product_weight: decimal(10,2) (nullable = true)\n",
      " |-- product_color: string (nullable = true)\n",
      " |-- product_size: string (nullable = true)\n",
      " |-- product_brand: string (nullable = true)\n",
      " |-- product_material: string (nullable = true)\n",
      " |-- product_description: string (nullable = true)\n",
      " |-- product_rating: decimal(10,3) (nullable = true)\n",
      " |-- product_reviews: integer (nullable = true)\n",
      " |-- product_release_date: timestamp (nullable = true)\n",
      " |-- product_expiry_date: timestamp (nullable = true)\n",
      " |-- supplier_name: string (nullable = true)\n",
      " |-- supplier_contact: string (nullable = true)\n",
      " |-- supplier_email: string (nullable = true)\n",
      " |-- supplier_phone: string (nullable = true)\n",
      " |-- supplier_address: string (nullable = true)\n",
      " |-- supplier_city: string (nullable = true)\n",
      " |-- supplier_country: string (nullable = true)\n",
      "\n",
      "+---+-------------------+------------------+------------+--------------------+----------------+--------------------+-----------------+-----------------+------------------+-----------------+----------------+--------------------+--------------+------------------+------------+----------------+-------------+----------------+-------------------+----------------+--------------+---------------+-------------+----------------+----------+--------------+----------+-----------+-------------+------------+--------------------+------------+--------------+-------------+------------+-------------+----------------+--------------------+--------------+---------------+--------------------+-------------------+-------------+----------------+--------------------+--------------+----------------+-------------+----------------+\n",
      "| id|customer_first_name|customer_last_name|customer_age|      customer_email|customer_country|customer_postal_code|customer_pet_type|customer_pet_name|customer_pet_breed|seller_first_name|seller_last_name|        seller_email|seller_country|seller_postal_code|product_name|product_category|product_price|product_quantity|          sale_date|sale_customer_id|sale_seller_id|sale_product_id|sale_quantity|sale_total_price|store_name|store_location|store_city|store_state|store_country| store_phone|         store_email|pet_category|product_weight|product_color|product_size|product_brand|product_material| product_description|product_rating|product_reviews|product_release_date|product_expiry_date|supplier_name|supplier_contact|      supplier_email|supplier_phone|supplier_address|supplier_city|supplier_country|\n",
      "+---+-------------------+------------------+------------+--------------------+----------------+--------------------+-----------------+-----------------+------------------+-----------------+----------------+--------------------+--------------+------------------+------------+----------------+-------------+----------------+-------------------+----------------+--------------+---------------+-------------+----------------+----------+--------------+----------+-----------+-------------+------------+--------------------+------------+--------------+-------------+------------+-------------+----------------+--------------------+--------------+---------------+--------------------+-------------------+-------------+----------------+--------------------+--------------+----------------+-------------+----------------+\n",
      "|  1|             Barron|           Rawlyns|          61|bmassingham0@army...|           China|                 nan|              cat|        Priscella|Labrador Retriever|            Bevan|      Massingham|bmassingham0@answ...|     Indonesia|               nan|    Dog Food|            Food|        77.97|              89|2021-05-14 00:00:00|               1|             1|              1|            4|          487.70|   Youopia|      Suite 75|   Xichehe|        nan|United States|564-244-8660|bmassingham0@netw...|        Cats|         13.40|       Indigo|      Medium|        Skajo|           Steel|Aliquam quis turp...|         2.100|             97| 2011-10-19 00:00:00|2028-10-21 00:00:00|       Tagcat|Bevan Massingham|bmassingham0@unbl...|  914-877-7062|        Suite 25|       Kletek|           China|\n",
      "+---+-------------------+------------------+------------+--------------------+----------------+--------------------+-----------------+-----------------+------------------+-----------------+----------------+--------------------+--------------+------------------+------------+----------------+-------------+----------------+-------------------+----------------+--------------+---------------+-------------+----------------+----------+--------------+----------+-----------+-------------+------------+--------------------+------------+--------------+-------------+------------+-------------+----------------+--------------------+--------------+---------------+--------------------+-------------------+-------------+----------------+--------------------+--------------+----------------+-------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.jdbc(pg_url, \"mock_data\", properties=pg_props)\n",
    "df.printSchema()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe6f17e-93da-4586-ad25-eba4290aded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "customers_df = df.select(\n",
    "    col(\"customer_first_name\").alias(\"first_name\"),\n",
    "    col(\"customer_last_name\").alias(\"last_name\"),\n",
    "    col(\"customer_age\").alias(\"age\"),\n",
    "    col(\"customer_email\").alias(\"email\"),\n",
    "    col(\"customer_country\").alias(\"country\"),\n",
    "    col(\"customer_postal_code\").alias(\"postal_code\"),\n",
    "    col(\"customer_pet_type\").alias(\"pet_type\"), \n",
    "    col(\"customer_pet_name\").alias(\"pet_name\"), \n",
    "    col(\"customer_pet_breed\").alias(\"pet_breed\"), \n",
    "    \"pet_category\"\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружаем в postgres в таблицу d_customers\n",
    "customers_df.write.jdbc(url=pg_url, table=\"d_customers\", mode=\"append\", properties=pg_props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f49632-b94a-428e-95e2-49f1e0aadf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "products_df = df.select(\n",
    "    col(\"product_name\").alias(\"name\"),\n",
    "    col(\"product_category\").alias(\"category\"),\n",
    "    col(\"product_price\").alias(\"price\"),\n",
    "    col(\"product_quantity\").alias(\"quantity\"),\n",
    "    col(\"product_weight\").alias(\"weight\"),\n",
    "    col(\"product_color\").alias(\"color\"),\n",
    "    col(\"product_size\").alias(\"size\"),\n",
    "    col(\"product_brand\").alias(\"brand\"),\n",
    "    col(\"product_material\").alias(\"material\"),\n",
    "    col(\"product_description\").alias(\"description\"),\n",
    "    col(\"product_rating\").alias(\"rating\"),\n",
    "    col(\"product_reviews\").alias(\"reviews\"),\n",
    "    col(\"product_release_date\").alias(\"release_date\"),\n",
    "    col(\"product_expiry_date\").alias(\"expiry_date\")\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружает в postgres в таблицу d_products\n",
    "products_df.write.jdbc(url=pg_url, table=\"d_products\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "474695bc-5d45-41bb-875a-78d03f849757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sellers_df = df.select(\n",
    "    col(\"seller_first_name\").alias(\"first_name\"),\n",
    "    col(\"seller_last_name\").alias(\"last_name\"),\n",
    "    col(\"seller_email\").alias(\"email\"),\n",
    "    col(\"seller_country\").alias(\"country\"),\n",
    "    col(\"seller_postal_code\").alias(\"postal_code\")\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружает в postgres в таблицу d_sellers\n",
    "sellers_df.write.jdbc(url=pg_url, table=\"d_sellers\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0b3c9f-a218-460e-82f1-b7824c299d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "suppliers_df = df.select(\n",
    "    col(\"supplier_name\").alias(\"name\"),\n",
    "    col(\"supplier_contact\").alias(\"contact\"),\n",
    "    col(\"supplier_email\").alias(\"email\"),\n",
    "    col(\"supplier_phone\").alias(\"phone\"),\n",
    "    col(\"supplier_address\").alias(\"address\"),\n",
    "    col(\"supplier_city\").alias(\"city\"),\n",
    "    col(\"supplier_country\").alias(\"country\")\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружает в postgres в таблицу d_suppliers\n",
    "suppliers_df.write.jdbc(url=pg_url, table=\"d_suppliers\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853ae569-0cb0-4318-95cd-2fc20765392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "stores_df = df.select(\n",
    "    col(\"store_name\").alias(\"name\"),\n",
    "    col(\"store_location\").alias(\"location\"),\n",
    "    col(\"store_city\").alias(\"city\"),\n",
    "    col(\"store_state\").alias(\"state\"),\n",
    "    col(\"store_country\").alias(\"country\"),\n",
    "    col(\"store_phone\").alias(\"phone\"),\n",
    "    col(\"store_email\").alias(\"email\")\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружает в postgres в таблицу d_stores\n",
    "stores_df.write.jdbc(url=pg_url, table=\"d_stores\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51b85f09-17cd-4cc4-9690-4ac7ba7e58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, dayofweek\n",
    "\n",
    "times_df = df.select(\n",
    "    col(\"sale_date\").alias(\"date\")\n",
    ").dropDuplicates()\n",
    "\n",
    "times_df = times_df.withColumn(\"year\", year(\"date\")) \\\n",
    "                   .withColumn(\"month\", month(\"date\")) \\\n",
    "                   .withColumn(\"day\", dayofmonth(\"date\")) \\\n",
    "                   .withColumn(\"weekday\", dayofweek(\"date\"))\n",
    "\n",
    "# загружает в postgres в таблицу d_times\n",
    "times_df.write.jdbc(url=pg_url, table=\"d_times\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdefe9e-7e9a-4a02-bef5-e44507fb2498",
   "metadata": {},
   "source": [
    "Теперь свяжем все данные с таблицей фактов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16708f39-e932-4987-beeb-40bff0f95f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# берем данные мз таблиц измерений\n",
    "dim_customers = spark.read.jdbc(url=pg_url, table=\"d_customers\", properties=pg_props)\n",
    "dim_sellers = spark.read.jdbc(url=pg_url, table=\"d_sellers\", properties=pg_props)\n",
    "dim_products = spark.read.jdbc(url=pg_url, table=\"d_products\", properties=pg_props)\n",
    "dim_stores = spark.read.jdbc(url=pg_url, table=\"d_stores\", properties=pg_props)\n",
    "dim_suppliers = spark.read.jdbc(url=pg_url, table=\"d_suppliers\", properties=pg_props)\n",
    "dim_times = spark.read.jdbc(url=pg_url, table=\"d_times\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053a608-080b-4e45-8b15-c7c299b45c70",
   "metadata": {},
   "source": [
    "Теперь соединим данные в соответсвии с исходной таблицей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e3a00b4-4aca-46fd-9970-6dc277cd194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, broadcast, monotonically_increasing_id\n",
    "\n",
    "# alias для читаемости\n",
    "raw = df.alias(\"raw\")\n",
    "cust = dim_customers.alias(\"cust\")\n",
    "sell = dim_sellers.alias(\"sell\")\n",
    "prod = dim_products.alias(\"prod\")\n",
    "store = dim_stores.alias(\"store\")\n",
    "sup = dim_suppliers.alias(\"sup\")\n",
    "dt = dim_times.alias(\"dt\")\n",
    "\n",
    "# JOIN всех измерений\n",
    "fact_df = (\n",
    "    raw\n",
    "    .join(cust,\n",
    "          (raw[\"customer_email\"] == cust[\"email\"]),\n",
    "          \"left\")\n",
    "    .join(sell,\n",
    "          (raw[\"seller_email\"] == sell[\"email\"]),\n",
    "          \"left\")\n",
    "    .join(prod,\n",
    "          (raw[\"product_name\"] == prod[\"name\"]) &\n",
    "          (raw[\"product_category\"] == prod[\"category\"]) &\n",
    "          (raw[\"product_price\"] == prod[\"price\"]) &\n",
    "          (raw[\"product_brand\"] == prod[\"brand\"]) &\n",
    "          (raw[\"product_size\"] == prod[\"size\"]) &\n",
    "          (raw[\"product_material\"] == prod[\"material\"]),\n",
    "          \"left\")\n",
    "    .join(store,\n",
    "          (raw[\"store_email\"] == store[\"email\"]),\n",
    "          \"left\")\n",
    "    .join(sup,\n",
    "          (raw[\"supplier_email\"] == sup[\"email\"]),\n",
    "          \"left\")\n",
    "    .join(broadcast(dt), raw[\"sale_date\"] == dt[\"date\"], \"left\")\n",
    "    .select(\n",
    "        col(\"dt.id\").alias(\"date_id\"),\n",
    "        col(\"cust.id\").alias(\"customer_id\"),\n",
    "        col(\"sell.id\").alias(\"seller_id\"),\n",
    "        col(\"prod.id\").alias(\"product_id\"),\n",
    "        col(\"store.id\").alias(\"store_id\"),\n",
    "        col(\"sup.id\").alias(\"supplier_id\"),\n",
    "        col(\"raw.sale_quantity\").cast(\"int\").alias(\"quantity\"),\n",
    "        col(\"raw.sale_total_price\").cast(\"decimal(10,2)\").alias(\"total_price\")\n",
    "    )\n",
    ")\n",
    "print(fact_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdbbe2-c2c6-4596-8cc0-b8db5c104cbb",
   "metadata": {},
   "source": [
    "На этапе join возникли проблемы, так как делал слияния с ошибкой --- не уникальные поля брал, в следствие чего получил ~11000000 записей в `f_sales` и долгую запись."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f8cb9-a960-4633-9c2d-edb4e084574d",
   "metadata": {},
   "source": [
    "Отбираем только нужные столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21c4865e-11c6-4ef9-83cc-8a514e2d4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df = fact_df.repartition(4)\n",
    "\n",
    "# Запись с настройками\n",
    "fact_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", pg_url) \\\n",
    "    .option(\"dbtable\", \"f_sales\") \\\n",
    "    .option(\"user\", pg_props[\"user\"]) \\\n",
    "    .option(\"password\", pg_props[\"password\"]) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"batchsize\", \"500\") \\\n",
    "    .option(\"truncate\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1891b-4b5a-4f69-9c28-a3d055a2fa03",
   "metadata": {},
   "source": [
    "В данном случае нет необходимости использовать батчевую загрузку, 10000 не так много."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc0153-2f98-48fe-8679-7c5b2cdb73b2",
   "metadata": {},
   "source": [
    "## Работа с clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199acd6a-1b9d-4c03-9619-1861fcfdd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark report to clickhouse\") \\\n",
    "    .config(\"spark.jars\", \"clickhouse-jdbc-0.4.6.jar,postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "ch_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_props = {\n",
    "    \"user\": \"custom_user\",\n",
    "    \"password\": \"pswd\",\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "}\n",
    "\n",
    "pg_url = \"jdbc:postgresql://bigdata_postgres_db:5432/lab2\"\n",
    "pg_props = {\n",
    "    \"user\": \"debug\",\n",
    "    \"password\": \"pswd\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3048f8e-7932-4b07-92c6-b67b6c6d79f5",
   "metadata": {},
   "source": [
    "### Отчет по продажам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d41e3a-08ae-4828-9de4-38662add8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "\n",
    "dm_products = spark.read.jdbc(url=pg_url, table=\"d_products\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b06bf-9f14-4498-80ce-c2166b833620",
   "metadata": {},
   "source": [
    "Достаем топ-10 самых продаваемых продуктов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a882a25a-357d-42c9-b5b6-07b09b720a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|     name|total_quantity|\n",
      "+---------+--------------+\n",
      "| Dog Food|         18298|\n",
      "|Bird Cage|         18205|\n",
      "|  Cat Toy|         18120|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "top_products = (\n",
    "    fact_df\n",
    "    .join(dm_products, fact_df.product_id == dm_products.id)\n",
    "    .groupBy(dm_products[\"name\"])\n",
    "    .agg(_sum(fact_df[\"quantity\"]).alias(\"total_quantity\"))\n",
    "    .orderBy(col(\"total_quantity\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941541c-592f-4a23-b0dc-c179b0d4424b",
   "metadata": {},
   "source": [
    "Общая выручка по категориям продуктов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0702d34b-b909-49f9-a802-e4021c103f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|category|total_revenue|\n",
      "+--------+-------------+\n",
      "|     Toy|    868101.63|\n",
      "|    Cage|    831117.94|\n",
      "|    Food|    830632.55|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, col\n",
    "\n",
    "revenue_by_category = (\n",
    "    fact_df\n",
    "    .join(dm_products, fact_df.product_id == dm_products.id)\n",
    "    .groupBy(dm_products[\"category\"])\n",
    "    .agg(_sum(fact_df[\"total_price\"]).alias(\"total_revenue\"))\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    ")\n",
    "\n",
    "revenue_by_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729ed5a-0df9-4c59-bc7d-e9830bdf698f",
   "metadata": {},
   "source": [
    "Средний рейтинг и количество отзывов для каждого продукта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6df739ec-c70a-4f5a-9a44-bc8b39359d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+\n",
      "|     name|avg_rating|total_reviews|\n",
      "+---------+----------+-------------+\n",
      "|Bird Cage| 3.0001492|      1682260|\n",
      "| Dog Food| 3.0182989|      1653413|\n",
      "|  Cat Toy| 3.0068601|      1676222|\n",
      "+---------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, max, sum\n",
    "\n",
    "ratings_reviews = (\n",
    "    dm_products\n",
    "    .groupBy(\"name\")\n",
    "    .agg(\n",
    "        avg(\"rating\").alias(\"avg_rating\"),\n",
    "        sum(\"reviews\").alias(\"total_reviews\")\n",
    "    )\n",
    ")\n",
    "ratings_reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fd8a99c-a1cc-47e1-b711-aab53070bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "df_top10_json = top_products \\\n",
    "    .withColumn(\"report_type\", lit(\"top_10_products\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*top_products.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_revenue_json = revenue_by_category \\\n",
    "    .withColumn(\"report_type\", lit(\"revenue_by_category\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*revenue_by_category.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_ratings_json = ratings_reviews \\\n",
    "    .withColumn(\"report_type\", lit(\"ratings_reviews\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*ratings_reviews.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_reports = df_top10_json.union(df_revenue_json).union(df_ratings_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "341b5d2f-974b-41fd-9f13-5bcb1d26e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports.write.mode(\"append\").jdbc(url=ch_url, table=\"report_product\", properties=ch_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204464a8-ff84-4f7e-82b8-833e4909c4dd",
   "metadata": {},
   "source": [
    "### Отчёт по клиентам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87f5a59d-298a-4ead-a293-e597b8114df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sales = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "d_customers = spark.read.jdbc(url=pg_url, table=\"d_customers\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0a559-fac8-4f11-948c-df7583cc8191",
   "metadata": {},
   "source": [
    "Топ-10 клиентов с наибольшей общей суммой покупок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cd0cd6e-e606-42a7-bfa6-413a22c49f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+-----------+\n",
      "|customer_id|first_name| last_name|  country|total_spent|\n",
      "+-----------+----------+----------+---------+-----------+\n",
      "|       5408|       Gus| Hartshorn|  Albania|     499.85|\n",
      "|       1770|     Hayes|    McKain| Portugal|     499.80|\n",
      "|       4593|       Ava|     Lomas|    China|     499.76|\n",
      "|       4423|     Dawna|     Impey|Indonesia|     499.76|\n",
      "|       3357|   Lavinia| Horsburgh|   Poland|     499.73|\n",
      "|       8018|      Dame|Auchinleck|Indonesia|     499.71|\n",
      "|       1048|  Isahella|    Colley|   Russia|     499.69|\n",
      "|       5937|     Nicky|    Lattie|   Mexico|     499.62|\n",
      "|       3025|    Sisely|  Bonevant|    China|     499.62|\n",
      "|       5130|      Eran|     Cotes|    China|     499.59|\n",
      "+-----------+----------+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "top_customers = (\n",
    "    f_sales.join(d_customers, f_sales.customer_id == d_customers.id)\n",
    "    .groupBy(\"customer_id\", \"first_name\", \"last_name\", \"country\")\n",
    "    .agg(_sum(\"total_price\").alias(\"total_spent\"))\n",
    "    .orderBy(\"total_spent\", ascending=False)\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "top_customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddf156-b5ed-4a60-b76a-f266826d0814",
   "metadata": {},
   "source": [
    "Распределение клиентов по странам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4298f9d4-c394-410e-947d-28b43f7fcf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             country|count_of_customers|\n",
      "+--------------------+------------------+\n",
      "|                Chad|                 5|\n",
      "|              Russia|               628|\n",
      "|            Paraguay|                18|\n",
      "|               Yemen|                39|\n",
      "| U.S. Virgin Islands|                 1|\n",
      "|             Senegal|                 4|\n",
      "|              Sweden|               264|\n",
      "|Svalbard and Jan ...|                 1|\n",
      "|              Guyana|                 1|\n",
      "|         Philippines|               555|\n",
      "|             Eritrea|                 3|\n",
      "|            Djibouti|                 1|\n",
      "|            Malaysia|                40|\n",
      "|              Turkey|                 1|\n",
      "|              Malawi|                12|\n",
      "|                Iraq|                 8|\n",
      "|             Germany|                30|\n",
      "|Northern Mariana ...|                 1|\n",
      "|             Comoros|                13|\n",
      "|         Afghanistan|                31|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "country_distribution = (\n",
    "    d_customers.groupBy(\"country\").agg(count(\"*\").alias(\"count_of_customers\"))\n",
    ")\n",
    "\n",
    "country_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7c709-cf69-4342-84d7-cac132e67d2c",
   "metadata": {},
   "source": [
    "Средний чек для каждого клиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c33bec4-c3c8-446b-b08b-8e6366bd3e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+\n",
      "|customer_id|first_name| last_name|avg_check|\n",
      "+-----------+----------+----------+---------+\n",
      "|        148|    Sander|  Warhurst|   308.63|\n",
      "|        463|       Dov|   Stanton|   372.40|\n",
      "|        471|     Hedda|   Enrrico|   334.62|\n",
      "|        496|   Sheeree|  Matthias|   383.82|\n",
      "|        833|   Merrick| Shotboult|   377.44|\n",
      "|       1088|      Cori|    Spragg|   463.65|\n",
      "|       1238|      Rois|   Byfford|    65.36|\n",
      "|       1342|      Alla|      Jore|   365.57|\n",
      "|       1580|     Tarah|   Scanlin|    14.63|\n",
      "|       1591|    Jasper|   Antonat|   378.88|\n",
      "|       1645|  Tomasina|     Bound|   163.33|\n",
      "|       1829|       Eve|   Cheshir|   480.87|\n",
      "|       1959|   Donovan|     Toupe|   302.61|\n",
      "|       2122|   Camilla|    Kieran|    83.58|\n",
      "|       2142|   Corenda|    Allatt|    28.17|\n",
      "|       2366|    Livvie|  Mountjoy|    64.11|\n",
      "|       2659|   Carilyn|      Glyn|    71.86|\n",
      "|       2866|    Roarke|     Kobus|    16.43|\n",
      "|       3175|    Tracey|McAllaster|   230.77|\n",
      "|       3749|   Madelon|    Tomlin|   190.75|\n",
      "+-----------+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum, round\n",
    "\n",
    "avg_check = (\n",
    "    f_sales.join(d_customers, f_sales.customer_id == d_customers.id).groupBy(\"customer_id\", \"first_name\", \"last_name\")\n",
    "    .agg(round(_sum(\"total_price\") / count(f_sales.id), 2).alias(\"avg_check\"))\n",
    ")\n",
    "\n",
    "avg_check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf526e-4268-49eb-8ebc-6752c2140da7",
   "metadata": {},
   "source": [
    "Объединяем фреймы в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "565d4a84-62c9-4809-ac3e-4feb15bffdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "df_top_customers_json = top_customers \\\n",
    "    .withColumn(\"report_type\", lit(\"top_10_customers\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*top_customers.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_distribution_json = country_distribution \\\n",
    "    .withColumn(\"report_type\", lit(\"distribution_by_country\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*country_distribution.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_avg_check_json = avg_check \\\n",
    "    .withColumn(\"report_type\", lit(\"avg_check\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*avg_check.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_reports = df_top_customers_json.union(df_distribution_json).union(df_avg_check_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "126b85e1-a0fe-41af-9fd2-33ed4cf8a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports.write.mode(\"append\").jdbc(url=ch_url, table=\"report_customer\", properties=ch_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b582d-d4d1-439e-8293-d52ef2ff09ee",
   "metadata": {},
   "source": [
    "###  Отчёт. Витрина продаж по времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "983af5e8-6c4d-43e3-98b6-e5e38a6131a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sales = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "d_times = spark.read.jdbc(url=pg_url, table=\"d_times\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d74292-a04b-4bf4-855d-7526089eda00",
   "metadata": {},
   "source": [
    "Месячные и годовые тренды продаж."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c70963dd-b18d-4359-a578-9a7142c12626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+------------------+\n",
      "|year|month|total_revenue|year_total_revenue|\n",
      "+----+-----+-------------+------------------+\n",
      "|2021|    1|    224158.54|        2529852.12|\n",
      "|2021|    2|    192348.31|        2529852.12|\n",
      "|2021|    3|    207282.20|        2529852.12|\n",
      "|2021|    4|    206592.82|        2529852.12|\n",
      "|2021|    5|    211764.86|        2529852.12|\n",
      "|2021|    6|    215042.80|        2529852.12|\n",
      "|2021|    7|    220496.51|        2529852.12|\n",
      "|2021|    8|    221275.78|        2529852.12|\n",
      "|2021|    9|    210623.43|        2529852.12|\n",
      "|2021|   10|    228743.32|        2529852.12|\n",
      "|2021|   11|    200154.69|        2529852.12|\n",
      "|2021|   12|    191368.86|        2529852.12|\n",
      "+----+-----+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# обычная агрегация по году и месяцу\n",
    "monthly_yearly_trends = (\n",
    "    f_sales.join(d_times, f_sales.date_id == d_times.id)\n",
    "    .groupBy(\"year\", \"month\")\n",
    "    .agg(_sum(\"total_price\").alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "# окно по каждому году\n",
    "year_window = Window.partitionBy(\"year\")\n",
    "\n",
    "# добавим столбец с годовой выручкой\n",
    "monthly_with_year_total = monthly_yearly_trends.withColumn(\n",
    "    \"year_total_revenue\",\n",
    "    _sum(\"total_revenue\").over(year_window)\n",
    ").orderBy(\"year\", \"month\")\n",
    "\n",
    "monthly_with_year_total.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56dcb7e-9db9-49f4-b8e2-c40a445f2e7a",
   "metadata": {},
   "source": [
    "Сравнение выручки за разные периоды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "16e7c201-3acf-4583-9d81-7d441dc86037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+-----------+\n",
      "|season|total_revenue|orders_count|avg_revenue|\n",
      "+------+-------------+------------+-----------+\n",
      "|winter|    607875.71|        2383|     255.09|\n",
      "|summer|    656815.09|        2577|     254.88|\n",
      "|spring|    625639.88|        2508|     249.46|\n",
      "|autumn|    639521.44|        2532|     252.58|\n",
      "+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, count, round, when\n",
    "\n",
    "\n",
    "d_times_with_season = d_times.withColumn(\n",
    "    \"season\",\n",
    "    when((d_times[\"month\"].isin(12, 1, 2)), \"winter\")\n",
    "    .when((d_times[\"month\"].isin(3, 4, 5)), \"spring\")\n",
    "    .when((d_times[\"month\"].isin(6, 7, 8)), \"summer\")\n",
    "    .when((d_times[\"month\"].isin(9, 10, 11)), \"autumn\")\n",
    ")\n",
    "\n",
    "avg_revenue_by_season = (\n",
    "    f_sales.join(d_times_with_season, f_sales.date_id == d_times_with_season.id)\n",
    "    .groupBy(\"season\")\n",
    "    .agg(\n",
    "        _sum(\"total_price\").alias(\"total_revenue\"),\n",
    "        count(f_sales.id).alias(\"orders_count\"),\n",
    "        round(_sum(\"total_price\") / count(f_sales.id), 2).alias(\"avg_revenue\")\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "avg_revenue_by_season.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3636f-163c-49c7-b007-a7b59578302b",
   "metadata": {},
   "source": [
    "Средний размер заказа по месяцам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b7b38-9e9e-4ce7-b976-42fa44deb5e9",
   "metadata": {},
   "source": [
    "*Так как у нас данные за один год 2021, то не вывожу его.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "be943b2e-7cfe-45b5-8643-c041cc96f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------------+--------------+\n",
      "|month|total_revenue|orders_count|avg_order_size|\n",
      "+-----+-------------+------------+--------------+\n",
      "|    1|    224158.54|         874|        256.47|\n",
      "|    2|    192348.31|         739|        260.28|\n",
      "|    3|    207282.20|         843|        245.89|\n",
      "|    4|    206592.82|         837|        246.83|\n",
      "|    5|    211764.86|         828|        255.75|\n",
      "|    6|    215042.80|         822|        261.61|\n",
      "|    7|    220496.51|         858|        256.99|\n",
      "|    8|    221275.78|         897|        246.68|\n",
      "|    9|    210623.43|         839|        251.04|\n",
      "|   10|    228743.32|         892|        256.44|\n",
      "|   11|    200154.69|         801|        249.88|\n",
      "|   12|    191368.86|         770|        248.53|\n",
      "+-----+-------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, count, round, col\n",
    "\n",
    "avg_order_by_month = (\n",
    "    f_sales.join(d_times, f_sales.date_id == d_times.id)\n",
    "    .groupBy(\"month\")\n",
    "    .agg(\n",
    "        _sum(\"total_price\").alias(\"total_revenue\"),\n",
    "        count(f_sales.id).alias(\"orders_count\"),\n",
    "        round(_sum(\"total_price\") / count(f_sales.id), 2).alias(\"avg_order_size\")\n",
    "    )\n",
    "    .orderBy(\"month\")\n",
    ")\n",
    "\n",
    "avg_order_by_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a73815-7b1f-4844-8b7b-766644747766",
   "metadata": {},
   "source": [
    "Собираем аналитику вместе и сохраняем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d51bc8e2-1e1b-44db-beca-e9b2e875c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "\n",
    "df_revenue_json = monthly_with_year_total \\\n",
    "    .withColumn(\"report_type\", lit(\"revenue_by_month_and_year\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*monthly_with_year_total.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_avg_order_json = avg_order_by_month \\\n",
    "    .withColumn(\"report_type\", lit(\"avg_order_by_month\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*avg_order_by_month.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_avg_season_json = avg_revenue_by_season \\\n",
    "    .withColumn(\"report_type\", lit(\"avg_revenue_by_season\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*avg_revenue_by_season.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_reports = df_revenue_json.union(df_avg_order_json).union(df_avg_season_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5739a795-3538-4e84-b4b3-f0051563be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports.write.mode(\"append\").jdbc(url=ch_url, table=\"report_time\", properties=ch_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e6869-a1e1-432b-9bca-b89f13829587",
   "metadata": {},
   "source": [
    "### Витрина продаж по магазинам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7542fb81-0460-4321-9f77-e6a03573fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sales = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "d_stores = spark.read.jdbc(url=pg_url, table=\"d_stores\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b0991-e1ec-4034-af79-3ad9d0740007",
   "metadata": {},
   "source": [
    "Топ-5 магазинов с наибольшей выручкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba99e37d-227b-4211-9265-5a0dd65707bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------+-------+\n",
      "|       name|     city|     country|revenue|\n",
      "+-----------+---------+------------+-------+\n",
      "|       DabZ|   Grekan|South Africa| 499.85|\n",
      "|Thoughtblab|    Fonte|      Poland| 499.80|\n",
      "|   Edgeblab|    Pesek|   Indonesia| 499.76|\n",
      "|     Camido|Longzhong|      Sweden| 499.76|\n",
      "|    Centizu|   Tylicz|      Poland| 499.73|\n",
      "+-----------+---------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "top_5_stores = (\n",
    "    f_sales.groupBy(\"store_id\")\n",
    "    .agg(sum(\"total_price\").alias(\"revenue\"))\n",
    "    .join(d_stores, f_sales.store_id == d_stores.id)\n",
    "    .select(\"name\", \"city\", \"country\", \"revenue\")\n",
    "    .orderBy(\"revenue\", ascending=False)\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "top_5_stores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31457d-3e80-4544-b361-b52bb070f188",
   "metadata": {},
   "source": [
    "Распределение продаж по городам и странам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a27363-97ff-4987-811b-a4ad2d4e7504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+--------------+\n",
      "|    country|         city|total_revenue|total_quantity|\n",
      "+-----------+-------------+-------------+--------------+\n",
      "|Afghanistan|     Alingsås|       166.11|             4|\n",
      "|Afghanistan|    Asheville|       466.53|            10|\n",
      "|Afghanistan|        Asker|       375.17|             2|\n",
      "|Afghanistan|   Berkovitsa|       167.99|            10|\n",
      "|Afghanistan|     Borūjerd|       492.22|            10|\n",
      "|Afghanistan|Calzada Larga|       457.36|             3|\n",
      "|Afghanistan| Cruz del Eje|       187.23|             5|\n",
      "|Afghanistan|       Daugai|       485.66|             5|\n",
      "|Afghanistan|     Ddmashen|        65.99|             2|\n",
      "|Afghanistan|   El Rosario|       359.37|             6|\n",
      "|Afghanistan|      Gagarin|       301.95|             9|\n",
      "|Afghanistan|      Gjinkar|       387.76|            10|\n",
      "|Afghanistan|Golema Rečica|       385.67|             4|\n",
      "|Afghanistan| Grand Rapids|       241.88|             8|\n",
      "|Afghanistan|      Guankou|       160.80|             2|\n",
      "|Afghanistan|     Göteborg|       330.82|             3|\n",
      "|Afghanistan|  Huangnaihai|       399.70|             6|\n",
      "|Afghanistan|   Huaranchal|       320.28|             1|\n",
      "|Afghanistan|       Huimin|       264.16|             2|\n",
      "|Afghanistan|     Jiangchi|       420.43|             7|\n",
      "+-----------+-------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "sales_by_location = (\n",
    "    f_sales.join(d_stores, f_sales.store_id == d_stores.id)\n",
    "    .groupBy(\"country\", \"city\")\n",
    "    .agg(sum(\"total_price\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\"))\n",
    "    .orderBy(\"country\", \"city\")\n",
    ")\n",
    "\n",
    "sales_by_location.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13cd96d-feb6-4fe5-bd6c-7a2a8765c279",
   "metadata": {},
   "source": [
    "Средний чек для каждого магазина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fdffc48-9046-476f-b825-58de60a74a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|         name|avg_check|\n",
      "+-------------+---------+\n",
      "|         Mymm|   360.04|\n",
      "|         Jayo|   172.76|\n",
      "|     Livetube|    46.63|\n",
      "|       Skimia|   174.15|\n",
      "|      Voolith|    14.11|\n",
      "|        Vimbo|    39.42|\n",
      "| Jabbersphere|   195.98|\n",
      "|      Dabvine|   308.35|\n",
      "|     Gigazoom|   399.54|\n",
      "|       Voolia|   238.61|\n",
      "|   Topicshots|   443.13|\n",
      "|      Pixonyx|   325.13|\n",
      "|        Aimbu|   237.52|\n",
      "|      Dabtype|   101.05|\n",
      "|      Gabcube|   499.21|\n",
      "|        Eadel|    32.03|\n",
      "|Twitternation|   273.09|\n",
      "|    Bubblebox|   164.21|\n",
      "|         Lajo|   478.60|\n",
      "|  Shufflester|   446.64|\n",
      "+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, count, round, col\n",
    "\n",
    "avg_check_by_store = (\n",
    "    f_sales.join(d_stores, f_sales.store_id == d_stores.id)\n",
    "    .groupBy(d_stores.id, \"name\")\n",
    "    .agg(\n",
    "        _sum(\"total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"orders_count\"),\n",
    "    )\n",
    "    .withColumn(\"avg_check\", round(col(\"total_revenue\") / col(\"orders_count\"), 2))\n",
    "    .select(\"name\", \"avg_check\")\n",
    ")\n",
    "\n",
    "avg_check_by_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e3331e6-0fdd-4aa9-b7ef-5bba5807cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "df_top5_json = top_5_stores \\\n",
    "    .withColumn(\"report_type\", lit(\"top_5_stores\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*top_5_stores.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_sales_location_json = sales_by_location \\\n",
    "    .withColumn(\"report_type\", lit(\"sales_by_location\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*sales_by_location.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_avg_check_json = avg_check_by_store \\\n",
    "    .withColumn(\"report_type\", lit(\"avg_check_by_store\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*avg_check_by_store.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_store_reports = df_top5_json.union(df_sales_location_json).union(df_avg_check_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d5adfb-8fd9-4b02-b993-3346e64904b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_reports.write.mode(\"append\").jdbc(url=ch_url, table=\"store_report\", properties=ch_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ea197-2be1-4c88-aaa6-beec3a7bfade",
   "metadata": {},
   "source": [
    "### Продажи по поставщикам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a4bf8ac-8baf-4f31-b795-b3e5d33bc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sales = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "d_suppliers = spark.read.jdbc(url=pg_url, table=\"d_suppliers\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b38e73-a4c6-48b2-b167-8655a4556bb0",
   "metadata": {},
   "source": [
    "Топ-5 поставщиков с наибольшей выручкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a78273aa-e1f8-4ef5-9af1-0c6cc9a124b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+-------+\n",
      "|      name|      city|  country|revenue|\n",
      "+----------+----------+---------+-------+\n",
      "|Brainverse|    Tudela|  Ireland| 499.85|\n",
      "|     Jamia|     Luleå|   Russia| 499.80|\n",
      "|     Eabox|   Sloboda| Portugal| 499.76|\n",
      "|   Demimbu|   Begejci|    China| 499.76|\n",
      "|Browsezoom|San Isidro|Argentina| 499.73|\n",
      "+----------+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "top_5_suppliers = (\n",
    "    f_sales.groupBy(\"supplier_id\")\n",
    "    .agg(sum(\"total_price\").alias(\"revenue\"))\n",
    "    .join(d_suppliers, f_sales.supplier_id == d_suppliers.id)\n",
    "    .select(\"name\", \"city\", \"country\", \"revenue\")\n",
    "    .orderBy(\"revenue\", ascending=False)\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "top_5_suppliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1071a9b-4489-4583-9bbe-f86c5c0f2b30",
   "metadata": {},
   "source": [
    "Средняя цена товаров от каждого поставщика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a22f2453-6c49-4e6f-a7ad-29fa6a60c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_products = spark.read.jdbc(url=pg_url, table=\"d_products\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a42cf727-9fd8-44ef-9930-bcf39fe0910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------------+\n",
      "|supplier_id|       name|    country|avg_product_price|\n",
      "+-----------+-----------+-----------+-----------------+\n",
      "|       5429|   Skipfire|Philippines|        87.030000|\n",
      "|       3008|      Mycat|     Brazil|        50.420000|\n",
      "|       2114|    Youfeed|  Indonesia|        39.160000|\n",
      "|       8231|      Jatri|      Tonga|        24.760000|\n",
      "|       8042|      Aivee|     Brazil|        93.890000|\n",
      "|       1105|     Avavee|      China|        78.290000|\n",
      "|       4738|      Plajo|      China|        48.230000|\n",
      "|       8849|    Youspan|     Poland|        90.620000|\n",
      "|       9248|  Flipstorm|   Portugal|        15.720000|\n",
      "|       3451|      Mynte|  Indonesia|        67.920000|\n",
      "|       1545| Thoughtmix|    Nigeria|        99.270000|\n",
      "|       3025|Jabberstorm|      China|        19.440000|\n",
      "|       4688|    Tagfeed|  Indonesia|         9.170000|\n",
      "|       6584|      Eabox|  Macedonia|        37.480000|\n",
      "|       4886|   Gigaclub|  Indonesia|        11.610000|\n",
      "|        930|   Linktype|     Poland|        40.920000|\n",
      "|       4190|     Quamba|   Slovenia|        96.430000|\n",
      "|       2118|      Abata|     Poland|        19.780000|\n",
      "|       9545|      Gevee|     Russia|        11.230000|\n",
      "|       2753|      Zooxo|     Russia|        93.750000|\n",
      "+-----------+-----------+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_price_by_supplier = (\n",
    "    f_sales.join(d_products, f_sales.product_id == d_products.id)\n",
    "           .join(d_suppliers, f_sales.supplier_id == d_suppliers.id)\n",
    "           .groupBy(\"supplier_id\", d_suppliers.name, \"country\")\n",
    "           .agg(avg(\"price\").alias(\"avg_product_price\"))\n",
    ")\n",
    "\n",
    "avg_price_by_supplier.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4bf892-c13b-44b1-ba63-3677730023f9",
   "metadata": {},
   "source": [
    "Распределение продаж по странам поставщиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d80c65ab-c5a2-479a-9dd8-6a6b91080147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|       country|  revenue|\n",
      "+--------------+---------+\n",
      "|         China|492823.31|\n",
      "|     Indonesia|265717.99|\n",
      "|        Russia|149206.75|\n",
      "|   Philippines|136135.10|\n",
      "|        Brazil| 97546.82|\n",
      "|        Poland| 87370.64|\n",
      "|      Portugal| 83210.60|\n",
      "|        France| 80432.46|\n",
      "| United States| 52560.14|\n",
      "|        Sweden| 52074.94|\n",
      "|Czech Republic| 45258.81|\n",
      "|       Ukraine| 42858.17|\n",
      "|      Thailand| 42409.05|\n",
      "|         Japan| 42075.85|\n",
      "|      Colombia| 39525.61|\n",
      "|          Peru| 37351.86|\n",
      "|     Argentina| 35606.32|\n",
      "|        Greece| 33146.62|\n",
      "|        Canada| 30290.58|\n",
      "|       Nigeria| 24725.21|\n",
      "+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "sales_by_supplier_country = (\n",
    "    f_sales.join(d_suppliers, f_sales.supplier_id == d_suppliers.id)\n",
    "    .groupBy(\"country\")\n",
    "    .agg(sum(\"total_price\").alias(\"revenue\"))\n",
    "    .orderBy(\"revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "sales_by_supplier_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "063d8b73-877a-4947-bbaa-c4377dfd8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "df_top5_json = top_5_suppliers \\\n",
    "    .withColumn(\"report_type\", lit(\"top_5_suppliers\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*top_5_suppliers.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_avg_price_json = avg_price_by_supplier \\\n",
    "    .withColumn(\"report_type\", lit(\"avg_product_price\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*avg_price_by_supplier.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_sales_country_json = sales_by_supplier_country \\\n",
    "    .withColumn(\"report_type\", lit(\"sales_by_supplier_country\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*sales_by_supplier_country.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "supplier_report = df_top5_json.union(df_avg_price_json).union(df_sales_country_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dd57d2a-249f-44bb-ae5b-d526cc93b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_report.write.mode(\"append\").jdbc(url=ch_url, table=\"report_supplier\", properties=ch_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bca8ca-e355-4ab9-8701-e1420634a790",
   "metadata": {},
   "source": [
    "### Качество продукции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52717ded-7560-4fc1-b124-29b3d4237664",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sales = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "d_products = spark.read.jdbc(url=pg_url, table=\"d_products\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a0a2a-a99d-4b1d-989b-849ae6c7cf41",
   "metadata": {},
   "source": [
    "Продукты с наивысшим и наименьшим рейтингом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6663d18-b62a-4d30-a59a-5cf6d9d9196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+\n",
      "|  id|     name|rating|\n",
      "+----+---------+------+\n",
      "| 586|  Cat Toy| 5.000|\n",
      "| 985|Bird Cage| 5.000|\n",
      "| 706|  Cat Toy| 5.000|\n",
      "|8023|  Cat Toy| 5.000|\n",
      "| 912| Dog Food| 5.000|\n",
      "+----+---------+------+\n",
      "\n",
      "+---+---------+------+\n",
      "| id|     name|rating|\n",
      "+---+---------+------+\n",
      "|171|Bird Cage| 1.000|\n",
      "|550| Dog Food| 1.000|\n",
      "|233| Dog Food| 1.000|\n",
      "|376| Dog Food| 1.000|\n",
      "| 35|  Cat Toy| 1.000|\n",
      "+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "top_rating_products = (\n",
    "    d_products.select(\"id\", \"name\", \"rating\").orderBy(\"rating\", ascending=False).limit(5)\n",
    ")\n",
    "\n",
    "lowest_rating_products = (\n",
    "    d_products.select(\"id\", \"name\", \"rating\").orderBy(\"rating\", ascending=True).limit(5)\n",
    ")\n",
    "\n",
    "top_rating_products.show()\n",
    "lowest_rating_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc220a6-890a-44b5-9a21-9e4fca8226ab",
   "metadata": {},
   "source": [
    "Корреляция между рейтингом и объемом продаж."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57c26c1a-ec14-4365-9708-0147555e257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              metric|               value|\n",
      "+--------------------+--------------------+\n",
      "|rating_vs_sales_c...|0.001004977801145763|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql import Row\n",
    "\n",
    "product_sales = (\n",
    "    f_sales.groupBy(\"product_id\")\n",
    "           .agg(sum(\"quantity\").alias(\"total_quantity\"))\n",
    ")\n",
    "\n",
    "rating_vs_sales = (\n",
    "    product_sales.join(d_products, product_sales.product_id == d_products.id)\n",
    "                 .select(\"rating\", \"total_quantity\")\n",
    ")\n",
    "\n",
    "correlation = rating_vs_sales.stat.corr(\"rating\", \"total_quantity\")\n",
    "correlation_df = spark.createDataFrame(\n",
    "    [Row(metric=\"rating_vs_sales_correlation\", value=correlation)]\n",
    ")\n",
    "\n",
    "correlation_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da725510-6f62-4914-80ce-e78626b77796",
   "metadata": {},
   "source": [
    "Продукты с наибольшим количеством отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "134dc566-2b44-44e0-9771-a38d515495c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+\n",
      "|  id|     name|reviews|\n",
      "+----+---------+-------+\n",
      "|4122|  Cat Toy|   1000|\n",
      "|5256|Bird Cage|   1000|\n",
      "|4224|Bird Cage|   1000|\n",
      "|3505|  Cat Toy|   1000|\n",
      "|4606|Bird Cage|   1000|\n",
      "+----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_reviewed_products = (\n",
    "    d_products.select(\"id\", \"name\", \"reviews\").orderBy(\"reviews\", ascending=False).limit(5)\n",
    ")\n",
    "\n",
    "most_reviewed_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4051287-7c03-44e3-95df-587e0b08fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "correlation_json = correlation_df \\\n",
    "    .withColumn(\"report_type\", lit(\"rating_vs_sales_correlation\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*correlation_df.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "top_rated_json = top_rating_products \\\n",
    "    .withColumn(\"report_type\", lit(\"top_rating_products\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*top_rating_products.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "low_rated_json = lowest_rating_products \\\n",
    "    .withColumn(\"report_type\", lit(\"lowest_rating_products\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*lowest_rating_products.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "most_reviews_json = most_reviewed_products \\\n",
    "    .withColumn(\"report_type\", lit(\"most_reviewed_products\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*most_reviewed_products.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "product_quality_report = (correlation_json.union(top_rated_json).union(low_rated_json).union(most_reviews_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8109476b-bfb0-4566-b355-b1b889a68b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_quality_report.write.mode(\"append\").jdbc(url=ch_url, table=\"report_quality\", properties=ch_props)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
