{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month, monotonically_increasing_id\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL to Star Schema\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/work/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pg_url = \"jdbc:postgresql://bigdata_postgres_db:5432/lab2\"\n",
    "pg_props = {\n",
    "    \"user\": \"debug\",\n",
    "    \"password\": \"pswd\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f965d639-3c81-4264-b06f-21e09a3f0ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- customer_first_name: string (nullable = true)\n",
      " |-- customer_last_name: string (nullable = true)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_country: string (nullable = true)\n",
      " |-- customer_postal_code: string (nullable = true)\n",
      " |-- customer_pet_type: string (nullable = true)\n",
      " |-- customer_pet_name: string (nullable = true)\n",
      " |-- customer_pet_breed: string (nullable = true)\n",
      " |-- seller_first_name: string (nullable = true)\n",
      " |-- seller_last_name: string (nullable = true)\n",
      " |-- seller_email: string (nullable = true)\n",
      " |-- seller_country: string (nullable = true)\n",
      " |-- seller_postal_code: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_price: decimal(10,2) (nullable = true)\n",
      " |-- product_quantity: integer (nullable = true)\n",
      " |-- sale_date: timestamp (nullable = true)\n",
      " |-- sale_customer_id: integer (nullable = true)\n",
      " |-- sale_seller_id: integer (nullable = true)\n",
      " |-- sale_product_id: integer (nullable = true)\n",
      " |-- sale_quantity: integer (nullable = true)\n",
      " |-- sale_total_price: decimal(10,2) (nullable = true)\n",
      " |-- store_name: string (nullable = true)\n",
      " |-- store_location: string (nullable = true)\n",
      " |-- store_city: string (nullable = true)\n",
      " |-- store_state: string (nullable = true)\n",
      " |-- store_country: string (nullable = true)\n",
      " |-- store_phone: string (nullable = true)\n",
      " |-- store_email: string (nullable = true)\n",
      " |-- pet_category: string (nullable = true)\n",
      " |-- product_weight: decimal(10,2) (nullable = true)\n",
      " |-- product_color: string (nullable = true)\n",
      " |-- product_size: string (nullable = true)\n",
      " |-- product_brand: string (nullable = true)\n",
      " |-- product_material: string (nullable = true)\n",
      " |-- product_description: string (nullable = true)\n",
      " |-- product_rating: decimal(10,3) (nullable = true)\n",
      " |-- product_reviews: integer (nullable = true)\n",
      " |-- product_release_date: timestamp (nullable = true)\n",
      " |-- product_expiry_date: timestamp (nullable = true)\n",
      " |-- supplier_name: string (nullable = true)\n",
      " |-- supplier_contact: string (nullable = true)\n",
      " |-- supplier_email: string (nullable = true)\n",
      " |-- supplier_phone: string (nullable = true)\n",
      " |-- supplier_address: string (nullable = true)\n",
      " |-- supplier_city: string (nullable = true)\n",
      " |-- supplier_country: string (nullable = true)\n",
      "\n",
      "+---+-------------------+------------------+------------+--------------------+----------------+--------------------+-----------------+-----------------+------------------+-----------------+----------------+--------------------+--------------+------------------+------------+----------------+-------------+----------------+-------------------+----------------+--------------+---------------+-------------+----------------+----------+--------------+----------+-----------+-------------+------------+--------------------+------------+--------------+-------------+------------+-------------+----------------+--------------------+--------------+---------------+--------------------+-------------------+-------------+----------------+--------------------+--------------+----------------+-------------+----------------+\n",
      "| id|customer_first_name|customer_last_name|customer_age|      customer_email|customer_country|customer_postal_code|customer_pet_type|customer_pet_name|customer_pet_breed|seller_first_name|seller_last_name|        seller_email|seller_country|seller_postal_code|product_name|product_category|product_price|product_quantity|          sale_date|sale_customer_id|sale_seller_id|sale_product_id|sale_quantity|sale_total_price|store_name|store_location|store_city|store_state|store_country| store_phone|         store_email|pet_category|product_weight|product_color|product_size|product_brand|product_material| product_description|product_rating|product_reviews|product_release_date|product_expiry_date|supplier_name|supplier_contact|      supplier_email|supplier_phone|supplier_address|supplier_city|supplier_country|\n",
      "+---+-------------------+------------------+------------+--------------------+----------------+--------------------+-----------------+-----------------+------------------+-----------------+----------------+--------------------+--------------+------------------+------------+----------------+-------------+----------------+-------------------+----------------+--------------+---------------+-------------+----------------+----------+--------------+----------+-----------+-------------+------------+--------------------+------------+--------------+-------------+------------+-------------+----------------+--------------------+--------------+---------------+--------------------+-------------------+-------------+----------------+--------------------+--------------+----------------+-------------+----------------+\n",
      "|  1|             Barron|           Rawlyns|          61|bmassingham0@army...|           China|                 nan|              cat|        Priscella|Labrador Retriever|            Bevan|      Massingham|bmassingham0@answ...|     Indonesia|               nan|    Dog Food|            Food|        77.97|              89|2021-05-14 00:00:00|               1|             1|              1|            4|          487.70|   Youopia|      Suite 75|   Xichehe|        nan|United States|564-244-8660|bmassingham0@netw...|        Cats|         13.40|       Indigo|      Medium|        Skajo|           Steel|Aliquam quis turp...|         2.100|             97| 2011-10-19 00:00:00|2028-10-21 00:00:00|       Tagcat|Bevan Massingham|bmassingham0@unbl...|  914-877-7062|        Suite 25|       Kletek|           China|\n",
      "+---+-------------------+------------------+------------+--------------------+----------------+--------------------+-----------------+-----------------+------------------+-----------------+----------------+--------------------+--------------+------------------+------------+----------------+-------------+----------------+-------------------+----------------+--------------+---------------+-------------+----------------+----------+--------------+----------+-----------+-------------+------------+--------------------+------------+--------------+-------------+------------+-------------+----------------+--------------------+--------------+---------------+--------------------+-------------------+-------------+----------------+--------------------+--------------+----------------+-------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.jdbc(pg_url, \"mock_data\", properties=pg_props)\n",
    "df.printSchema()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe6f17e-93da-4586-ad25-eba4290aded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "customers_df = df.select(\n",
    "    col(\"customer_first_name\").alias(\"first_name\"),\n",
    "    col(\"customer_last_name\").alias(\"last_name\"),\n",
    "    col(\"customer_age\").alias(\"age\"),\n",
    "    col(\"customer_email\").alias(\"email\"),\n",
    "    col(\"customer_country\").alias(\"country\"),\n",
    "    col(\"customer_postal_code\").alias(\"postal_code\"),\n",
    "    col(\"customer_pet_type\").alias(\"pet_type\"), \n",
    "    col(\"customer_pet_name\").alias(\"pet_name\"), \n",
    "    col(\"customer_pet_breed\").alias(\"pet_breed\"), \n",
    "    \"pet_category\"\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружаем в postgres в таблицу d_customers\n",
    "customers_df.write.jdbc(url=pg_url, table=\"d_customers\", mode=\"append\", properties=pg_props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f49632-b94a-428e-95e2-49f1e0aadf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "products_df = df.select(\n",
    "    col(\"product_name\").alias(\"name\"),\n",
    "    col(\"product_category\").alias(\"category\"),\n",
    "    col(\"product_price\").alias(\"price\"),\n",
    "    col(\"product_quantity\").alias(\"quantity\"),\n",
    "    col(\"product_weight\").alias(\"weight\"),\n",
    "    col(\"product_color\").alias(\"color\"),\n",
    "    col(\"product_size\").alias(\"size\"),\n",
    "    col(\"product_brand\").alias(\"brand\"),\n",
    "    col(\"product_material\").alias(\"material\"),\n",
    "    col(\"product_description\").alias(\"description\"),\n",
    "    col(\"product_rating\").alias(\"rating\"),\n",
    "    col(\"product_reviews\").alias(\"reviews\"),\n",
    "    col(\"product_release_date\").alias(\"release_date\"),\n",
    "    col(\"product_expiry_date\").alias(\"expiry_date\")\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружает в postgres в таблицу d_products\n",
    "products_df.write.jdbc(url=pg_url, table=\"d_products\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "474695bc-5d45-41bb-875a-78d03f849757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sellers_df = df.select(\n",
    "    col(\"seller_first_name\").alias(\"first_name\"),\n",
    "    col(\"seller_last_name\").alias(\"last_name\"),\n",
    "    col(\"seller_email\").alias(\"email\"),\n",
    "    col(\"seller_country\").alias(\"country\"),\n",
    "    col(\"seller_postal_code\").alias(\"postal_code\")\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружает в postgres в таблицу d_sellers\n",
    "sellers_df.write.jdbc(url=pg_url, table=\"d_sellers\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0b3c9f-a218-460e-82f1-b7824c299d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "suppliers_df = df.select(\n",
    "    col(\"supplier_name\").alias(\"name\"),\n",
    "    col(\"supplier_contact\").alias(\"contact\"),\n",
    "    col(\"supplier_email\").alias(\"email\"),\n",
    "    col(\"supplier_phone\").alias(\"phone\"),\n",
    "    col(\"supplier_address\").alias(\"address\"),\n",
    "    col(\"supplier_city\").alias(\"city\"),\n",
    "    col(\"supplier_country\").alias(\"country\")\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружает в postgres в таблицу d_suppliers\n",
    "suppliers_df.write.jdbc(url=pg_url, table=\"d_suppliers\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853ae569-0cb0-4318-95cd-2fc20765392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "stores_df = df.select(\n",
    "    col(\"store_name\").alias(\"name\"),\n",
    "    col(\"store_location\").alias(\"location\"),\n",
    "    col(\"store_city\").alias(\"city\"),\n",
    "    col(\"store_state\").alias(\"state\"),\n",
    "    col(\"store_country\").alias(\"country\"),\n",
    "    col(\"store_phone\").alias(\"phone\"),\n",
    "    col(\"store_email\").alias(\"email\")\n",
    ").dropDuplicates()\n",
    "\n",
    "# загружает в postgres в таблицу d_stores\n",
    "stores_df.write.jdbc(url=pg_url, table=\"d_stores\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51b85f09-17cd-4cc4-9690-4ac7ba7e58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, dayofweek\n",
    "\n",
    "times_df = df.select(\n",
    "    col(\"sale_date\").alias(\"date\")\n",
    ").dropDuplicates()\n",
    "\n",
    "times_df = times_df.withColumn(\"year\", year(\"date\")) \\\n",
    "                   .withColumn(\"month\", month(\"date\")) \\\n",
    "                   .withColumn(\"day\", dayofmonth(\"date\")) \\\n",
    "                   .withColumn(\"weekday\", dayofweek(\"date\"))\n",
    "\n",
    "# загружает в postgres в таблицу d_times\n",
    "times_df.write.jdbc(url=pg_url, table=\"d_times\", mode=\"append\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdefe9e-7e9a-4a02-bef5-e44507fb2498",
   "metadata": {},
   "source": [
    "Теперь свяжем все данные с таблицей фактов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16708f39-e932-4987-beeb-40bff0f95f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# берем данные мз таблиц измерений\n",
    "dim_customers = spark.read.jdbc(url=pg_url, table=\"d_customers\", properties=pg_props)\n",
    "dim_sellers = spark.read.jdbc(url=pg_url, table=\"d_sellers\", properties=pg_props)\n",
    "dim_products = spark.read.jdbc(url=pg_url, table=\"d_products\", properties=pg_props)\n",
    "dim_stores = spark.read.jdbc(url=pg_url, table=\"d_stores\", properties=pg_props)\n",
    "dim_suppliers = spark.read.jdbc(url=pg_url, table=\"d_suppliers\", properties=pg_props)\n",
    "dim_times = spark.read.jdbc(url=pg_url, table=\"d_times\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053a608-080b-4e45-8b15-c7c299b45c70",
   "metadata": {},
   "source": [
    "Теперь соединим данные в соответсвии с исходной таблицей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e3a00b4-4aca-46fd-9970-6dc277cd194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, broadcast, monotonically_increasing_id\n",
    "\n",
    "# alias для читаемости\n",
    "raw = df.alias(\"raw\")\n",
    "cust = dim_customers.alias(\"cust\")\n",
    "sell = dim_sellers.alias(\"sell\")\n",
    "prod = dim_products.alias(\"prod\")\n",
    "store = dim_stores.alias(\"store\")\n",
    "sup = dim_suppliers.alias(\"sup\")\n",
    "dt = dim_times.alias(\"dt\")\n",
    "\n",
    "# JOIN всех измерений\n",
    "fact_df = (\n",
    "    raw\n",
    "    .join(cust,\n",
    "          (raw[\"customer_email\"] == cust[\"email\"]),\n",
    "          \"left\")\n",
    "    .join(sell,\n",
    "          (raw[\"seller_email\"] == sell[\"email\"]),\n",
    "          \"left\")\n",
    "    .join(prod,\n",
    "          (raw[\"product_name\"] == prod[\"name\"]) &\n",
    "          (raw[\"product_category\"] == prod[\"category\"]) &\n",
    "          (raw[\"product_price\"] == prod[\"price\"]) &\n",
    "          (raw[\"product_brand\"] == prod[\"brand\"]) &\n",
    "          (raw[\"product_size\"] == prod[\"size\"]) &\n",
    "          (raw[\"product_material\"] == prod[\"material\"]),\n",
    "          \"left\")\n",
    "    .join(store,\n",
    "          (raw[\"store_email\"] == store[\"email\"]),\n",
    "          \"left\")\n",
    "    .join(sup,\n",
    "          (raw[\"supplier_email\"] == sup[\"email\"]),\n",
    "          \"left\")\n",
    "    .join(broadcast(dt), raw[\"sale_date\"] == dt[\"date\"], \"left\")\n",
    "    .select(\n",
    "        col(\"dt.id\").alias(\"date_id\"),\n",
    "        col(\"cust.id\").alias(\"customer_id\"),\n",
    "        col(\"sell.id\").alias(\"seller_id\"),\n",
    "        col(\"prod.id\").alias(\"product_id\"),\n",
    "        col(\"store.id\").alias(\"store_id\"),\n",
    "        col(\"sup.id\").alias(\"supplier_id\"),\n",
    "        col(\"raw.sale_quantity\").cast(\"int\").alias(\"quantity\"),\n",
    "        col(\"raw.sale_total_price\").cast(\"decimal(10,2)\").alias(\"total_price\")\n",
    "    )\n",
    ")\n",
    "print(fact_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdbbe2-c2c6-4596-8cc0-b8db5c104cbb",
   "metadata": {},
   "source": [
    "На этапе join возникли проблемы, так как делал слияния с ошибкой --- не уникальные поля брал, в следствие чего получил ~11000000 записей в `f_sales` и долгую запись."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f8cb9-a960-4633-9c2d-edb4e084574d",
   "metadata": {},
   "source": [
    "Отбираем только нужные столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21c4865e-11c6-4ef9-83cc-8a514e2d4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df = fact_df.repartition(4)\n",
    "\n",
    "# Запись с настройками\n",
    "fact_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", pg_url) \\\n",
    "    .option(\"dbtable\", \"f_sales\") \\\n",
    "    .option(\"user\", pg_props[\"user\"]) \\\n",
    "    .option(\"password\", pg_props[\"password\"]) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"batchsize\", \"500\") \\\n",
    "    .option(\"truncate\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1891b-4b5a-4f69-9c28-a3d055a2fa03",
   "metadata": {},
   "source": [
    "В данном случае нет необходимости использовать батчевую загрузку, 10000 не так много."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc0153-2f98-48fe-8679-7c5b2cdb73b2",
   "metadata": {},
   "source": [
    "## Работа с clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "199acd6a-1b9d-4c03-9619-1861fcfdd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark report to clickhouse\") \\\n",
    "    .config(\"spark.jars\", \"clickhouse-jdbc-0.4.6.jar,postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "ch_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_props = {\n",
    "    \"user\": \"custom_user\",\n",
    "    \"password\": \"pswd\",\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "}\n",
    "\n",
    "pg_url = \"jdbc:postgresql://bigdata_postgres_db:5432/lab2\"\n",
    "pg_props = {\n",
    "    \"user\": \"debug\",\n",
    "    \"password\": \"pswd\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3048f8e-7932-4b07-92c6-b67b6c6d79f5",
   "metadata": {},
   "source": [
    "### Отчет по продажам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d41e3a-08ae-4828-9de4-38662add8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "\n",
    "dm_products = spark.read.jdbc(url=pg_url, table=\"d_products\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b06bf-9f14-4498-80ce-c2166b833620",
   "metadata": {},
   "source": [
    "Достаем топ-10 самых продаваемых продуктов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a882a25a-357d-42c9-b5b6-07b09b720a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|     name|total_quantity|\n",
      "+---------+--------------+\n",
      "| Dog Food|         18298|\n",
      "|Bird Cage|         18205|\n",
      "|  Cat Toy|         18120|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "top_products = (\n",
    "    fact_df\n",
    "    .join(dm_products, fact_df.product_id == dm_products.id)\n",
    "    .groupBy(dm_products[\"name\"])\n",
    "    .agg(_sum(fact_df[\"quantity\"]).alias(\"total_quantity\"))\n",
    "    .orderBy(col(\"total_quantity\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941541c-592f-4a23-b0dc-c179b0d4424b",
   "metadata": {},
   "source": [
    "Общая выручка по категориям продуктов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0702d34b-b909-49f9-a802-e4021c103f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|category|total_revenue|\n",
      "+--------+-------------+\n",
      "|     Toy|    868101.63|\n",
      "|    Cage|    831117.94|\n",
      "|    Food|    830632.55|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, col\n",
    "\n",
    "revenue_by_category = (\n",
    "    fact_df\n",
    "    .join(dm_products, fact_df.product_id == dm_products.id)\n",
    "    .groupBy(dm_products[\"category\"])\n",
    "    .agg(_sum(fact_df[\"total_price\"]).alias(\"total_revenue\"))\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    ")\n",
    "\n",
    "revenue_by_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729ed5a-0df9-4c59-bc7d-e9830bdf698f",
   "metadata": {},
   "source": [
    "Средний рейтинг и количество отзывов для каждого продукта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6df739ec-c70a-4f5a-9a44-bc8b39359d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+\n",
      "|     name|avg_rating|total_reviews|\n",
      "+---------+----------+-------------+\n",
      "|Bird Cage| 3.0001492|      1682260|\n",
      "| Dog Food| 3.0182989|      1653413|\n",
      "|  Cat Toy| 3.0068601|      1676222|\n",
      "+---------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, max, sum\n",
    "\n",
    "ratings_reviews = (\n",
    "    dm_products\n",
    "    .groupBy(\"name\")\n",
    "    .agg(\n",
    "        avg(\"rating\").alias(\"avg_rating\"),\n",
    "        sum(\"reviews\").alias(\"total_reviews\")\n",
    "    )\n",
    ")\n",
    "ratings_reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fd8a99c-a1cc-47e1-b711-aab53070bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "df_top10_json = top_products \\\n",
    "    .withColumn(\"report_type\", lit(\"top_10_products\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*top_products.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_revenue_json = revenue_by_category \\\n",
    "    .withColumn(\"report_type\", lit(\"revenue_by_category\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*revenue_by_category.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_ratings_json = ratings_reviews \\\n",
    "    .withColumn(\"report_type\", lit(\"ratings_reviews\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*ratings_reviews.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_reports = df_top10_json.union(df_revenue_json).union(df_ratings_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "341b5d2f-974b-41fd-9f13-5bcb1d26e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports.write.mode(\"append\").jdbc(url=ch_url, table=\"report_product\", properties=ch_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204464a8-ff84-4f7e-82b8-833e4909c4dd",
   "metadata": {},
   "source": [
    "### Отчёт по клиентам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87f5a59d-298a-4ead-a293-e597b8114df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sales = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "d_customers = spark.read.jdbc(url=pg_url, table=\"d_customers\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0a559-fac8-4f11-948c-df7583cc8191",
   "metadata": {},
   "source": [
    "Топ-10 клиентов с наибольшей общей суммой покупок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cd0cd6e-e606-42a7-bfa6-413a22c49f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+-----------+\n",
      "|customer_id|first_name| last_name|  country|total_spent|\n",
      "+-----------+----------+----------+---------+-----------+\n",
      "|       5408|       Gus| Hartshorn|  Albania|     499.85|\n",
      "|       1770|     Hayes|    McKain| Portugal|     499.80|\n",
      "|       4593|       Ava|     Lomas|    China|     499.76|\n",
      "|       4423|     Dawna|     Impey|Indonesia|     499.76|\n",
      "|       3357|   Lavinia| Horsburgh|   Poland|     499.73|\n",
      "|       8018|      Dame|Auchinleck|Indonesia|     499.71|\n",
      "|       1048|  Isahella|    Colley|   Russia|     499.69|\n",
      "|       5937|     Nicky|    Lattie|   Mexico|     499.62|\n",
      "|       3025|    Sisely|  Bonevant|    China|     499.62|\n",
      "|       5130|      Eran|     Cotes|    China|     499.59|\n",
      "+-----------+----------+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "top_customers = (\n",
    "    f_sales.join(d_customers, f_sales.customer_id == d_customers.id)\n",
    "    .groupBy(\"customer_id\", \"first_name\", \"last_name\", \"country\")\n",
    "    .agg(_sum(\"total_price\").alias(\"total_spent\"))\n",
    "    .orderBy(\"total_spent\", ascending=False)\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "top_customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddf156-b5ed-4a60-b76a-f266826d0814",
   "metadata": {},
   "source": [
    "Распределение клиентов по странам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4298f9d4-c394-410e-947d-28b43f7fcf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             country|count_of_customers|\n",
      "+--------------------+------------------+\n",
      "|                Chad|                 5|\n",
      "|              Russia|               628|\n",
      "|            Paraguay|                18|\n",
      "|               Yemen|                39|\n",
      "| U.S. Virgin Islands|                 1|\n",
      "|             Senegal|                 4|\n",
      "|              Sweden|               264|\n",
      "|Svalbard and Jan ...|                 1|\n",
      "|              Guyana|                 1|\n",
      "|         Philippines|               555|\n",
      "|             Eritrea|                 3|\n",
      "|            Djibouti|                 1|\n",
      "|            Malaysia|                40|\n",
      "|              Turkey|                 1|\n",
      "|              Malawi|                12|\n",
      "|                Iraq|                 8|\n",
      "|             Germany|                30|\n",
      "|Northern Mariana ...|                 1|\n",
      "|             Comoros|                13|\n",
      "|         Afghanistan|                31|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "country_distribution = (\n",
    "    d_customers.groupBy(\"country\").agg(count(\"*\").alias(\"count_of_customers\"))\n",
    ")\n",
    "\n",
    "country_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7c709-cf69-4342-84d7-cac132e67d2c",
   "metadata": {},
   "source": [
    "Средний чек для каждого клиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c33bec4-c3c8-446b-b08b-8e6366bd3e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+\n",
      "|customer_id|first_name| last_name|avg_check|\n",
      "+-----------+----------+----------+---------+\n",
      "|        148|    Sander|  Warhurst|   308.63|\n",
      "|        463|       Dov|   Stanton|   372.40|\n",
      "|        471|     Hedda|   Enrrico|   334.62|\n",
      "|        496|   Sheeree|  Matthias|   383.82|\n",
      "|        833|   Merrick| Shotboult|   377.44|\n",
      "|       1088|      Cori|    Spragg|   463.65|\n",
      "|       1238|      Rois|   Byfford|    65.36|\n",
      "|       1342|      Alla|      Jore|   365.57|\n",
      "|       1580|     Tarah|   Scanlin|    14.63|\n",
      "|       1591|    Jasper|   Antonat|   378.88|\n",
      "|       1645|  Tomasina|     Bound|   163.33|\n",
      "|       1829|       Eve|   Cheshir|   480.87|\n",
      "|       1959|   Donovan|     Toupe|   302.61|\n",
      "|       2122|   Camilla|    Kieran|    83.58|\n",
      "|       2142|   Corenda|    Allatt|    28.17|\n",
      "|       2366|    Livvie|  Mountjoy|    64.11|\n",
      "|       2659|   Carilyn|      Glyn|    71.86|\n",
      "|       2866|    Roarke|     Kobus|    16.43|\n",
      "|       3175|    Tracey|McAllaster|   230.77|\n",
      "|       3749|   Madelon|    Tomlin|   190.75|\n",
      "+-----------+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum, round\n",
    "\n",
    "avg_check = (\n",
    "    f_sales.join(d_customers, f_sales.customer_id == d_customers.id).groupBy(\"customer_id\", \"first_name\", \"last_name\")\n",
    "    .agg(round(_sum(\"total_price\") / count(f_sales.id), 2).alias(\"avg_check\"))\n",
    ")\n",
    "\n",
    "avg_check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf526e-4268-49eb-8ebc-6752c2140da7",
   "metadata": {},
   "source": [
    "Объединяем фреймы в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "565d4a84-62c9-4809-ac3e-4feb15bffdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "df_top_customers_json = top_customers \\\n",
    "    .withColumn(\"report_type\", lit(\"top_10_customers\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*top_customers.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_distribution_json = country_distribution \\\n",
    "    .withColumn(\"report_type\", lit(\"distribution_by_country\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*country_distribution.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_avg_check_json = avg_check \\\n",
    "    .withColumn(\"report_type\", lit(\"avg_check\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*avg_check.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_reports = df_top_customers_json.union(df_distribution_json).union(df_avg_check_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "126b85e1-a0fe-41af-9fd2-33ed4cf8a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports.write.mode(\"append\").jdbc(url=ch_url, table=\"report_customer\", properties=ch_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b582d-d4d1-439e-8293-d52ef2ff09ee",
   "metadata": {},
   "source": [
    "###  Отчёт. Витрина продаж по времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "983af5e8-6c4d-43e3-98b6-e5e38a6131a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sales = spark.read.jdbc(url=pg_url, table=\"f_sales\", properties=pg_props)\n",
    "d_times = spark.read.jdbc(url=pg_url, table=\"d_times\", properties=pg_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d74292-a04b-4bf4-855d-7526089eda00",
   "metadata": {},
   "source": [
    "Месячные и годовые тренды продаж."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c70963dd-b18d-4359-a578-9a7142c12626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+------------------+\n",
      "|year|month|total_revenue|year_total_revenue|\n",
      "+----+-----+-------------+------------------+\n",
      "|2021|    1|    224158.54|        2529852.12|\n",
      "|2021|    2|    192348.31|        2529852.12|\n",
      "|2021|    3|    207282.20|        2529852.12|\n",
      "|2021|    4|    206592.82|        2529852.12|\n",
      "|2021|    5|    211764.86|        2529852.12|\n",
      "|2021|    6|    215042.80|        2529852.12|\n",
      "|2021|    7|    220496.51|        2529852.12|\n",
      "|2021|    8|    221275.78|        2529852.12|\n",
      "|2021|    9|    210623.43|        2529852.12|\n",
      "|2021|   10|    228743.32|        2529852.12|\n",
      "|2021|   11|    200154.69|        2529852.12|\n",
      "|2021|   12|    191368.86|        2529852.12|\n",
      "+----+-----+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# обычная агрегация по году и месяцу\n",
    "monthly_yearly_trends = (\n",
    "    f_sales.join(d_times, f_sales.date_id == d_times.id)\n",
    "    .groupBy(\"year\", \"month\")\n",
    "    .agg(_sum(\"total_price\").alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "# окно по каждому году\n",
    "year_window = Window.partitionBy(\"year\")\n",
    "\n",
    "# добавим столбец с годовой выручкой\n",
    "monthly_with_year_total = monthly_yearly_trends.withColumn(\n",
    "    \"year_total_revenue\",\n",
    "    _sum(\"total_revenue\").over(year_window)\n",
    ").orderBy(\"year\", \"month\")\n",
    "\n",
    "monthly_with_year_total.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56dcb7e-9db9-49f4-b8e2-c40a445f2e7a",
   "metadata": {},
   "source": [
    "Сравнение выручки за разные периоды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "16e7c201-3acf-4583-9d81-7d441dc86037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+-----------+\n",
      "|season|total_revenue|orders_count|avg_revenue|\n",
      "+------+-------------+------------+-----------+\n",
      "|winter|    607875.71|        2383|     255.09|\n",
      "|summer|    656815.09|        2577|     254.88|\n",
      "|spring|    625639.88|        2508|     249.46|\n",
      "|autumn|    639521.44|        2532|     252.58|\n",
      "+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, count, round, when\n",
    "\n",
    "\n",
    "d_times_with_season = d_times.withColumn(\n",
    "    \"season\",\n",
    "    when((d_times[\"month\"].isin(12, 1, 2)), \"winter\")\n",
    "    .when((d_times[\"month\"].isin(3, 4, 5)), \"spring\")\n",
    "    .when((d_times[\"month\"].isin(6, 7, 8)), \"summer\")\n",
    "    .when((d_times[\"month\"].isin(9, 10, 11)), \"autumn\")\n",
    ")\n",
    "\n",
    "avg_revenue_by_season = (\n",
    "    f_sales.join(d_times_with_season, f_sales.date_id == d_times_with_season.id)\n",
    "    .groupBy(\"season\")\n",
    "    .agg(\n",
    "        _sum(\"total_price\").alias(\"total_revenue\"),\n",
    "        count(f_sales.id).alias(\"orders_count\"),\n",
    "        round(_sum(\"total_price\") / count(f_sales.id), 2).alias(\"avg_revenue\")\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "avg_revenue_by_season.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3636f-163c-49c7-b007-a7b59578302b",
   "metadata": {},
   "source": [
    "Средний размер заказа по месяцам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b7b38-9e9e-4ce7-b976-42fa44deb5e9",
   "metadata": {},
   "source": [
    "*Так как у нас данные за один год 2021, то не вывожу его.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "be943b2e-7cfe-45b5-8643-c041cc96f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------------+--------------+\n",
      "|month|total_revenue|orders_count|avg_order_size|\n",
      "+-----+-------------+------------+--------------+\n",
      "|    1|    224158.54|         874|        256.47|\n",
      "|    2|    192348.31|         739|        260.28|\n",
      "|    3|    207282.20|         843|        245.89|\n",
      "|    4|    206592.82|         837|        246.83|\n",
      "|    5|    211764.86|         828|        255.75|\n",
      "|    6|    215042.80|         822|        261.61|\n",
      "|    7|    220496.51|         858|        256.99|\n",
      "|    8|    221275.78|         897|        246.68|\n",
      "|    9|    210623.43|         839|        251.04|\n",
      "|   10|    228743.32|         892|        256.44|\n",
      "|   11|    200154.69|         801|        249.88|\n",
      "|   12|    191368.86|         770|        248.53|\n",
      "+-----+-------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, count, round, col\n",
    "\n",
    "avg_order_by_month = (\n",
    "    f_sales.join(d_times, f_sales.date_id == d_times.id)\n",
    "    .groupBy(\"month\")\n",
    "    .agg(\n",
    "        _sum(\"total_price\").alias(\"total_revenue\"),\n",
    "        count(f_sales.id).alias(\"orders_count\"),\n",
    "        round(_sum(\"total_price\") / count(f_sales.id), 2).alias(\"avg_order_size\")\n",
    "    )\n",
    "    .orderBy(\"month\")\n",
    ")\n",
    "\n",
    "avg_order_by_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a73815-7b1f-4844-8b7b-766644747766",
   "metadata": {},
   "source": [
    "Собираем аналитику вместе и сохраняем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d51bc8e2-1e1b-44db-beca-e9b2e875c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_json, struct\n",
    "\n",
    "\n",
    "df_revenue_json = monthly_with_year_total \\\n",
    "    .withColumn(\"report_type\", lit(\"revenue_by_month\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*monthly_with_year_total.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_avg_order_json = avg_order_by_month \\\n",
    "    .withColumn(\"report_type\", lit(\"avg_order_by_month\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*avg_order_by_month.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_avg_season_json = avg_revenue_by_season \\\n",
    "    .withColumn(\"report_type\", lit(\"avg_revenue_by_season\")) \\\n",
    "    .withColumn(\"report_data\", to_json(struct(*avg_revenue_by_season.columns))) \\\n",
    "    .select(\"report_type\", \"report_data\")\n",
    "\n",
    "df_reports = df_revenue_json.union(df_avg_order_json).union(df_avg_season_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5739a795-3538-4e84-b4b3-f0051563be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports.write.mode(\"append\").jdbc(url=ch_url, table=\"report_time\", properties=ch_props)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
